{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 14:29:56 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from lighteval.models.transformers.transformers_model import TransformersModel, TransformersModelConfig\n",
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "BENCHMARKS = \"lighteval|gsm8k|5|1\" # suite|task|few_shot|truncate_few_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260178bd652413ba4ecba6a01dee8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "--max_samples WAS SET. THESE NUMBERS ARE ONLY PARTIAL AND SHOULD NOT BE USED FOR COMPARISON UNLESS YOU KNOW WHAT YOU ARE DOING.\n",
      "If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'gsm8k' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d67e8a8d844f4fadb913c2db2ccd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740c3f4738844700ab10bfb2c6896dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e246720284c64884a0db835586b0b554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be279c2f965c493b869f0a4a83f76327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7d89a615874d819463fc784eb888f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_tracker = EvaluationTracker(output_dir=\"./results\", save_details=True, push_to_hub=False)\n",
    "pipeline_params = PipelineParameters(\n",
    "    launcher_type=ParallelismManager.NONE,\n",
    "    max_samples=2\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  MODEL_NAME, device_map=\"auto\"\n",
    ")\n",
    "config = TransformersModelConfig(model_name=MODEL_NAME, batch_size=1)\n",
    "model = TransformersModel.from_model(model, config)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    model=model,\n",
    "    pipeline_parameters=pipeline_params,\n",
    "    evaluation_tracker=evaluation_tracker,\n",
    "    tasks=BENCHMARKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.\n",
      "Splits:   0%|          | 0/1 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Splits: 100%|██████████| 1/1 [01:34<00:00, 94.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      Task       |Version|     Metric     |Value|   |Stderr|\n",
      "|-----------------|------:|----------------|----:|---|-----:|\n",
      "|all              |       |extractive_match|    1|±  |     0|\n",
      "|lighteval:gsm8k:5|      0|extractive_match|    1|±  |     0|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = pipeline.evaluate()\n",
    "pipeline.show_results()\n",
    "results = pipeline.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_general': {'lighteval_sha': '?',\n",
       "  'num_fewshot_seeds': 1,\n",
       "  'max_samples': 2,\n",
       "  'job_id': 0,\n",
       "  'start_time': 8790.038662853,\n",
       "  'end_time': 8894.067931499,\n",
       "  'total_evaluation_time_secondes': '104.02926864599976',\n",
       "  'model_name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'model_sha': '8afb486c1db24fe5011ec46dfbe5b5dccdb575c2',\n",
       "  'model_dtype': torch.float32,\n",
       "  'model_size': '29.92 GB',\n",
       "  'generation_parameters': {},\n",
       "  'config': None},\n",
       " 'results': {'lighteval:gsm8k:5': defaultdict(float,\n",
       "              {'extractive_match': 1.0, 'extractive_match_stderr': 0.0}),\n",
       "  'all': {'extractive_match': 1.0, 'extractive_match_stderr': 0.0}},\n",
       " 'versions': {'lighteval:gsm8k:5': 0},\n",
       " 'config_tasks': {'lighteval:gsm8k': LightevalTaskConfig(name='gsm8k', prompt_function=<function gsm8k at 0x7337cebab1a0>, hf_repo='gsm8k', hf_subset='main', metric=(SampleLevelMetric(metric_name='extractive_match', higher_is_better=True, category=<MetricCategory.GENERATIVE: '3'>, use_case=<MetricUseCase.ACCURACY: '1'>, sample_level_fn=<function multilingual_extractive_match_metric.<locals>.sample_level_fn at 0x7337ce7e5120>, corpus_level_fn=<function mean at 0x73394af28130>),), hf_revision=None, hf_filter=None, hf_avail_splits=('train', 'test'), trust_dataset=True, evaluation_splits=('test',), few_shots_split=None, few_shots_select='random_sampling_from_train', generation_size=256, generation_grammar=None, stop_sequence=('Question:',), num_samples=None, suite=('lighteval',), original_num_docs=1319, effective_num_docs=2, must_remove_duplicate_docs=False, version=0)},\n",
       " 'summary_tasks': {'lighteval:gsm8k:5': DetailsLogger.CompiledDetail(hashes={'hash_examples': 'a986c90cf471729d', 'hash_full_prompts': '956e5c8e696aa7b4', 'hash_input_tokens': 'b8871ddff4bbda7e', 'hash_cont_tokens': '51223fd4eba73a67'}, truncated=2, non_truncated=0, padded=0, non_padded=2, effective_few_shots=5.0, num_truncated_few_shots=0)},\n",
       " 'summary_general': {'hashes': {'hash_examples': 'ad01334659b5dafc',\n",
       "   'hash_full_prompts': '7f86d8e1f158e56b',\n",
       "   'hash_input_tokens': '50d8e16146c1c931',\n",
       "   'hash_cont_tokens': '30bdfe8a50b62c35'},\n",
       "  'truncated': 2,\n",
       "  'non_truncated': 0,\n",
       "  'padded': 0,\n",
       "  'non_padded': 2,\n",
       "  'num_truncated_few_shots': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2026_02_28_APR2526_venv",
   "language": "python",
   "name": "2026_02_28_apr2526_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
