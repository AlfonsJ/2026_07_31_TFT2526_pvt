{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 16:11:12 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from lighteval.models.transformers.transformers_model import TransformersModel, TransformersModelConfig\n",
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "BENCHMARKS = \"lighteval|gsm8k|1|1\" # suite|task|few_shot|truncate_few_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d65d74b821c40ddb1e5793527541701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "--max_samples WAS SET. THESE NUMBERS ARE ONLY PARTIAL AND SHOULD NOT BE USED FOR COMPARISON UNLESS YOU KNOW WHAT YOU ARE DOING.\n",
      "If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'gsm8k' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    }
   ],
   "source": [
    "evaluation_tracker = EvaluationTracker(output_dir=\"./results\", save_details=True, push_to_hub=False)\n",
    "pipeline_params = PipelineParameters(\n",
    "    launcher_type=ParallelismManager.NONE,\n",
    "    max_samples=2\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  MODEL_NAME, device_map=\"auto\"\n",
    ")\n",
    "config = TransformersModelConfig(model_name=MODEL_NAME, batch_size=1)\n",
    "model = TransformersModel.from_model(model, config)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    model=model,\n",
    "    pipeline_parameters=pipeline_params,\n",
    "    evaluation_tracker=evaluation_tracker,\n",
    "    tasks=BENCHMARKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.\n",
      "Splits:   0%|          | 0/1 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Splits: 100%|██████████| 1/1 [02:04<00:00, 124.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      Task       |Version|     Metric     |Value|   |Stderr|\n",
      "|-----------------|------:|----------------|----:|---|-----:|\n",
      "|all              |       |extractive_match|  0.5|±  |   0.5|\n",
      "|lighteval:gsm8k:1|      0|extractive_match|  0.5|±  |   0.5|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = pipeline.evaluate()\n",
    "pipeline.show_results()\n",
    "results = pipeline.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_general': {'lighteval_sha': '?',\n",
       "  'num_fewshot_seeds': 1,\n",
       "  'max_samples': 2,\n",
       "  'job_id': 0,\n",
       "  'start_time': 14904.462102192,\n",
       "  'end_time': 15040.258730416,\n",
       "  'total_evaluation_time_secondes': '135.79662822400132',\n",
       "  'model_name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'model_sha': '8afb486c1db24fe5011ec46dfbe5b5dccdb575c2',\n",
       "  'model_dtype': torch.float32,\n",
       "  'model_size': '29.92 GB',\n",
       "  'generation_parameters': {},\n",
       "  'config': None},\n",
       " 'results': {'lighteval:gsm8k:1': defaultdict(float,\n",
       "              {'extractive_match': 0.5, 'extractive_match_stderr': 0.5}),\n",
       "  'all': {'extractive_match': 0.5, 'extractive_match_stderr': 0.5}},\n",
       " 'versions': {'lighteval:gsm8k:1': 0},\n",
       " 'config_tasks': {'lighteval:gsm8k': LightevalTaskConfig(name='gsm8k', prompt_function=<function gsm8k at 0x78c79a3cb240>, hf_repo='gsm8k', hf_subset='main', metric=(SampleLevelMetric(metric_name='extractive_match', higher_is_better=True, category=<MetricCategory.GENERATIVE: '3'>, use_case=<MetricUseCase.ACCURACY: '1'>, sample_level_fn=<function multilingual_extractive_match_metric.<locals>.sample_level_fn at 0x78c799e051c0>, corpus_level_fn=<function mean at 0x78c912730df0>),), hf_revision=None, hf_filter=None, hf_avail_splits=('train', 'test'), trust_dataset=True, evaluation_splits=('test',), few_shots_split=None, few_shots_select='random_sampling_from_train', generation_size=256, generation_grammar=None, stop_sequence=('Question:',), num_samples=None, suite=('lighteval',), original_num_docs=1319, effective_num_docs=2, must_remove_duplicate_docs=False, version=0)},\n",
       " 'summary_tasks': {'lighteval:gsm8k:1': DetailsLogger.CompiledDetail(hashes={'hash_examples': 'a986c90cf471729d', 'hash_full_prompts': '83e04b4840e8235e', 'hash_input_tokens': 'e7d9c324a04f9f8a', 'hash_cont_tokens': '071c6d282e2e18e0'}, truncated=2, non_truncated=0, padded=0, non_padded=2, effective_few_shots=1.0, num_truncated_few_shots=0)},\n",
       " 'summary_general': {'hashes': {'hash_examples': 'ad01334659b5dafc',\n",
       "   'hash_full_prompts': '3a1dd383945c998f',\n",
       "   'hash_input_tokens': '025608864651cd3b',\n",
       "   'hash_cont_tokens': '23aa2b57c4e1b860'},\n",
       "  'truncated': 2,\n",
       "  'non_truncated': 0,\n",
       "  'padded': 0,\n",
       "  'non_padded': 2,\n",
       "  'num_truncated_few_shots': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2026_02_28_APR2526_venv",
   "language": "python",
   "name": "2026_02_28_apr2526_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
